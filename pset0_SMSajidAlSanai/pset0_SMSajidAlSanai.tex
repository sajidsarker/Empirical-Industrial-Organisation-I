\documentclass{article}

\usepackage{amsmath}
\usepackage{listings}
\usepackage{color}

\title{Empirical Industrial Organisations I: PSet 0}
\author{S M Sajid Al Sanai}
\date{October 10, 2018}

\begin{document}

\maketitle
\pagenumbering{arabic}
\tableofcontents

\lstset{
	frame=lines,
	basicstyle=\small\sffamily,
	tabsize=4,
	columns=fixed,
	showstringspaces=false,
	showtabs=false,
	keepspaces,
	commentstyle=\color{red},
	keywordstyle=\color{blue}
}

\newpage
\section{Question I}
The Ordinary Least Squares Regression conducted through minimisation of the mean squared error included all regressors with the exclusion of Fixed Effect for Sunday to avoid perfect collinearity. The matrix of regressors $X_i$ includes a vector of ones in the initial column representing $x_0$. $\hat{\beta_0}$ represents the constant intercept.

\subsection{Minimisation with Python using FMin}
Minimisation by Python using $scipy.optimize.fmin()$ function yields the $\hat{\beta}$ regression coefficients detailed below. The initial guess vector for coefficients used was $(0, 0, 0, 0, 0, 0, 0, 0, 0)'$. FMin is the Python SciPy library equivalent to Matlab's FMinSearch routine.

% Code Snippet
\begin{lstlisting}
Warning: Maximum number of function evaluations has been exceeded.
[-0.3670506  -0.0036703   1.01575822 -0.42291809 -0.8131776   0.95473859
 -0.0401332  -0.44040212 -0.01237488]
Beta 0: -0.36705060338769724
Beta 1: -0.003670303573980958
Beta 2: 1.0157582164346053
Beta 3: -0.4229180860932915
Beta 4: -0.8131775992858263
Beta 5: 0.9547385945733078
Beta 6: -0.04013319650279834
Beta 7: -0.44040211745200697
Beta 8: -0.012374877796537835
\end{lstlisting}

\subsection{Minimisation with Python using Basin-Hopping}
Minimisation by Python using $scipy.optimize.basinhopping()$ function yields the $\hat{\beta}$ regression coefficients detailed below. The initial guess vector for coefficients used was $(0, 0, 0, 0, 0, 0, 0, 0, 0)'$, with a maximum of four iterations specified.

% Code Snippet
\begin{lstlisting}
[-1.26705331 -0.00313307  1.01656612  0.84355108 -0.07467264  1.01281306
  0.50899781 -0.8729692  -0.69728724]
Beta 0: -1.2670533120869718
Beta 1: -0.0031330720191008786
Beta 2: 1.0165661182808754
Beta 3: 0.8435510787546923
Beta 4: -0.07467264477145204
Beta 5: 1.0128130639764752
Beta 6: 0.5089978058816674
Beta 7: -0.8729691967245163
Beta 8: -0.6972872427184164
\end{lstlisting}

\subsection{Comparison to Regression using Stata}
Due to difficulty with typecasting in $statsmodels.api$, regression was unable to run in Python. The comparison was therefore done using Stata.

As seen in the code snippet of regression coefficients in Ordinary Least Squares Regression generated by Stata, we observe that the estimated coefficients on the regressors derived by the Basin-Hopping iterative method on Python have yielded the correct values as opposed to the FMin iterative method.

The function used was,$$reg\;ARR\_DELAY\;DISTANCE\;DEP\_DELAY\;FE\_MONDAY\;FE\_TUESDAY$$ $$FE\_WEDNESDAY\;FE\_THURSDAY\;FE\_FRIDAY\;FE\_SATURDAY$$

% Code Snippet
\begin{lstlisting}
      Source        SS       df       MS              Number of obs =   20412
-------------------------------------------           F(  8, 20403) =20889.16
       Model   35412319.9     8  4426539.99           Prob > F      =  0.0000
    Residual   4323520.36 20403   211.90611           R-squared     =  0.8912
-------------------------------------------           Adj R-squared =  0.8912
       Total   39735840.3 20411  1946.78557           Root MSE      =  14.557

-----------------------------------------------------------------------------
   ARR_DELAY       Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-----------------------------------------------------------------------------
    DISTANCE   -.0031331   .0001723   -18.18   0.000    -.0034708   -.0027953
   DEP_DELAY    1.016566   .0024917   407.98   0.000     1.011682     1.02145
   FE_MONDAY    .8435255   .3917058     2.15   0.031     .0757507      1.6113
  FE_TUESDAY   -.0746839    .387552    -0.19   0.847     -.834317    .6849492
FE_WEDNESDAY    1.012869   .3855522     2.63   0.009     .2571558    1.768582
 FE_THURSDAY    .5090663   .3785944     1.34   0.179    -.2330092    1.251142
   FE_FRIDAY   -.8729858   .3796207    -2.30   0.021    -1.617073   -.1288987
 FE_SATURDAY   -.6973007   .4111434    -1.70   0.090    -1.503175    .1085735
       _cons   -1.267056   .3148117    -4.02   0.000    -1.884112   -.6499993
\end{lstlisting}

\newpage
\section{Question II}
Given that the dependent variable created was a discrete binary variable for the late arrival of flights, it was necessary that the summation of log of probabilities was done for both cases of flight arrivals, late or otherwise. In order to do this, I used the dependent variable $arr\_late$ as a means to index which probability would be active for the particular observation.

When $arr\_late=0$, $(1-arr\_late)=1$ which is the same as evaluation to \textbf{True}. Conversely, when $arr\_late=1$, $(1-arr\_late)=0$ which is the same as evaluation to \textbf{False}. Therefore, the correct approach would be to multiply $(1-arr\_late)$ with the probability that the flight is not late, and $arr\_late$ with the probability that the flight is late  in each iterative step of the for-loop over which summation occurs.
$$L(\beta)=\sum\limits_{i=1}^N ln\left\{ (1-arr\_late)\times\Big(1-\frac{e^{\beta X_i}}{1+e^{\beta X_i}}\Big) + (arr\_late)\times\Big(\frac{e^{\beta X_i}}{1+e^{\beta X_i}}\Big)\right\} $$

\subsection{Optimisation with Python using FMin}
Optimisation by Python using $scipy.optimize.fmin()$ function yields the $\hat{\beta}$ regression coefficients detailed below. The initial guess vector for coefficients used was $(0, 0, 0)'$.

% Code Snippet
\begin{lstlisting}
Optimization terminated successfully.
         Current function value: 4987.810711
         Iterations: 142
         Function evaluations: 259
[-2.61299801e+00 -1.36749723e-04  1.29496431e-01]
Beta 0: -2.6129980089111475
Beta 1: -0.0001367497232092546
Beta 2: 0.12949643091507046
\end{lstlisting}

\subsection{Optimisation with Python using Minimise}
Optimisation by Python using $scipy.optimize.minimize()$ function yields the $\hat{\beta}$ regression coefficients detailed below. The initial guess vector for coefficients used was $(0, 0, 0)'$.

% Code Snippet
\begin{lstlisting}
        x: array([-2.61303099e+00, -1.36734311e-04,  1.29495648e-01])
Beta 0: -2.6130309909349796
Beta 1: -0.00013673431142185487
Beta 2: 0.12949564776587152
\end{lstlisting}

\subsection{Comparison to Logistic Regression using Stata}
As seen in the code snippet of regression coefficients in Logistic Regression generated by Stata, we observe that the estimated coefficients on the regressors derived by both the FMin and Minimisation iterative method on Python have yielded correct values.

Stata is able to provide both the coefficients and the odds ratio of our specified regressors. Both are detailed below. The functions used were,$$logistic\;ARR\_LATE\;DISTANCE\;DEP\_DELAY$$ $$logit\;ARR\_LATE\;DISTANCE\;DEP\_DELAY$$

% Code Snippet
\begin{lstlisting}
Logistic regression                               Number of obs   =      20412
                                                  LR chi2(2)      =   11857.58
                                                  Prob > chi2     =     0.0000
Log likelihood = -4987.8107                       Pseudo R2       =     0.5431

-----------------------------------------------------------------------------
    ARR_LATE  Odds Ratio   Std. Err.      z    P>|z|     [95% Conf. Interval]
-----------------------------------------------------------------------------
    DISTANCE    .9998633   .0000461    -2.96   0.003     .9997729    .9999537
   DEP_DELAY    1.138254   .0023709    62.17   0.000     1.133617     1.14291
       _cons    .0733107   .0036468   -52.53   0.000     .0665005    .0808184
-----------------------------------------------------------------------------
Note: 0 failures and 255 successes completely determined.

Iteration 0:   log likelihood = -10916.599  
Iteration 1:   log likelihood = -5100.9292  
Iteration 2:   log likelihood = -5000.6587  
Iteration 3:   log likelihood = -4987.8287  
Iteration 4:   log likelihood = -4987.8107  
Iteration 5:   log likelihood = -4987.8107  

Logistic regression                               Number of obs   =      20412
                                                  LR chi2(2)      =   11857.58
                                                  Prob > chi2     =     0.0000
Log likelihood = -4987.8107                       Pseudo R2       =     0.5431

-----------------------------------------------------------------------------
    ARR_LATE       Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-----------------------------------------------------------------------------
    DISTANCE   -.0001367   .0000461    -2.96   0.003    -.0002272   -.0000463
   DEP_DELAY    .1294956   .0020829    62.17   0.000     .1254132     .133578
       _cons   -2.613048   .0497444   -52.53   0.000    -2.710545   -2.515551
-----------------------------------------------------------------------------
Note: 0 failures and 255 successes completely determined.
\end{lstlisting}


\newpage
\section{Question III}
The Generalised Method of Moments Estimation required performing a minimisation on a criterion function. In the first stage, one uses the identity matrix as a weighting matrix in the first optimisation, followed by the use of estimated regression coefficients in the construction of a new weighting matrix for a second optimisation. This yields our 2-Stage Least Squares Estimators.

\subsection{First Stage Minimisation with Minimize}
The first stage function to be minimised was, $(Z'(Y-X\beta))'I_{4}(Z'(Y-X\beta))$ with $W=I_4$. From this, the weight matrix $W=\Sigma^{-1}(\hat{\beta})=(\sum \limits_{i=1}^N (Y-X\hat{\beta})_i^{2} z_i z'_i)^{-1}$ to be used in the second stage was calculated. The estimated errors represented the difference in the dependent variable $Y$ and estimated values $X\hat{\beta}$ using the resulting regression coefficients from minimisation with $W=I_4$.

The regression coefficients from the first stage optimisation using the Minimize iterative method $scipy.optimize.minimize()$ with an initial guess vector of $(0, 0, 0)'$ are as follows,

\begin{lstlisting}
        x: array([1.91868978, 1.10360019, 3.65262034])
Beta 0: 1.9186897768620108
Beta 1: 1.1036001911432256
Beta 2: 3.652620340522879
\end{lstlisting}

\subsection{Second Stage Minimisation with Minimize}
The second stage function to be minimised was $(Z'(Y-X\beta))'W(Z'(Y-X\beta))$ with $W=\Sigma^{-1}(\hat{\beta})=(\sum \limits_{i=1}^N (Y-X\hat{\beta})_i^{2} z_i z'_i)^{-1}$. Estimated errors were then recalculated as the difference between $Y$ and $X\hat{\beta}$.

The regression coefficients from the second stage optimisation using the Minimize iterative method $scipy.optimize.minimize()$ with an initial guess vector of $(0, 0, 0)'$ are as follows,

\begin{lstlisting}
        x: array([1.92104581, 1.09399361, 3.66145241])
Beta 0: 1.9210458074628922
Beta 1: 1.093993605947826
Beta 2: 3.661452412205812
\end{lstlisting}

\break
\subsection{Variance and Standard Errors}
The Variance and Standard Errors from the first stage are presented below,

\begin{lstlisting}
GMM Stage 1 Variance
[[ 0.034012   -0.00221611  0.00207005]
 [-0.00221611  0.03475512 -0.00921364]
 [ 0.00207005 -0.00921364  0.02753145]]

GMM Stage 1 Standard Errors
[[0.18442343        nan 0.04549784]
 [       nan 0.18642724        nan]
 [0.04549784        nan 0.16592603]]
\end{lstlisting}

\hfill \break
The Variance and Standard Errors from the second stage are presented below,

\begin{lstlisting}
GMM Stage 2 Variance
[[ 0.03401789 -0.0022231   0.00207841]
 [-0.0022231   0.03475173 -0.00921534]
 [ 0.00207841 -0.00921534  0.02752018]]

GMM Stage 2 Standard Errors
[[0.18443939        nan 0.04558958]
 [       nan 0.18641817        nan]
 [0.04558958        nan 0.16589207]]
\end{lstlisting}

\subsection{Comparison Between Stages}
The comparison between the Variance and Standard Errors matrices is the difference between that of the second and first stages. The difference in Variance and Standard Errors are presented below, with $nan$ implying $0$ or negligible, and the sign representing increase or decrease.

\begin{lstlisting}
GMM Variance Difference
[[ 1.59577010e-05             nan  9.17404363e-05]
 [            nan -9.06897682e-06             nan]
 [ 9.17404363e-05             nan -3.39656336e-05]]

GMM Standard Errors Difference
[[ 1.59577010e-05             nan  9.17404363e-05]
 [            nan -9.06897682e-06             nan]
 [ 9.17404363e-05             nan -3.39656336e-05]]
\end{lstlisting}

\newpage
\section{Appendix: Source Code}
\subsection{Source}

\lstinputlisting[language=Python]{pset0_SMSajidAlSanai.py}

\subsection{Output}

\begin{lstlisting}
 RESTART: C:\Users\Dell\Documents\Graduate - Economics\Empirical IO I\
	pset0_SMSajidAlSanai\pset0_SMSajidAlSanai.py 
Question 1:

N = 20412

Minimisation using FMin
Warning: Maximum number of function evaluations has been exceeded.
[-0.3670506  -0.0036703   1.01575822 -0.42291809 -0.8131776   0.95473859
 -0.0401332  -0.44040212 -0.01237488]
Beta 0: -0.36705060338769724
Beta 1: -0.003670303573980958
Beta 2: 1.0157582164346053
Beta 3: -0.4229180860932915
Beta 4: -0.8131775992858263
Beta 5: 0.9547385945733078
Beta 6: -0.04013319650279834
Beta 7: -0.44040211745200697
Beta 8: -0.012374877796537835

Minimisation using Basin-Hopping
[-1.26703813 -0.00313308  1.01656563  0.84356766 -0.07467908  1.01283905
  0.50906709 -0.87301486 -0.69732288]
Beta 0: -1.2670381300700035
Beta 1: -0.003133082060877313
Beta 2: 1.0165656290708287
Beta 3: 0.8435676647590163
Beta 4: -0.07467907921680436
Beta 5: 1.0128390477318374
Beta 6: 0.5090670858834021
Beta 7: -0.8730148589627836
Beta 8: -0.6973228763733291


Question 2:

Minimisation using FMin
Optimization terminated successfully.
         Current function value: 4987.810711
         Iterations: 142
         Function evaluations: 259
[-2.61299801e+00 -1.36749723e-04  1.29496431e-01]
Beta 0: -2.6129980089111475
Beta 1: -0.0001367497232092546
Beta 2: 0.12949643091507046

Minimisation using Minimise
      fun: 4987.810709922602
 hess_inv: array([[ 3.67335564e-06,  3.20140048e-08, -3.63433468e-06],
       [ 3.20140048e-08,  4.14347694e-09, -3.20864615e-08],
       [-3.63433468e-06, -3.20864615e-08,  3.63374805e-06]])
      jac: array([1.83105469e-04, 2.18688965e-01, 1.95312500e-03])
  message: 'Desired error not necessarily achieved due to precision loss.'
     nfev: 156
      nit: 19
     njev: 31
   status: 2
  success: False
        x: array([-2.61303099e+00, -1.36734311e-04,  1.29495648e-01])
Beta 0: -2.6130309909349796
Beta 1: -0.00013673431142185487
Beta 2: 0.12949564776587152


Question 3:

N = 1000

      fun: 21424.513095692088
 hess_inv: array([[ 7.38969124e-07, -1.79826688e-08,  3.32671680e-07],
       [-1.79826688e-08,  2.12417040e-07,  8.96671604e-08],
       [ 3.32671680e-07,  8.96671604e-08,  1.94939051e-07]])
      jac: array([0.00463867, 0.00634766, 0.00610352])
  message: 'Desired error not necessarily achieved due to precision loss.'
     nfev: 315
      nit: 8
     njev: 61
   status: 2
  success: False
        x: array([1.91868978, 1.10360019, 3.65262034])
Beta 0: 1.9186897768620108
Beta 1: 1.1036001911432256
Beta 2: 3.652620340522879

GMM Stage 1 Variance
[[ 0.034012   -0.00221611  0.00207005]
 [-0.00221611  0.03475512 -0.00921364]
 [ 0.00207005 -0.00921364  0.02753145]]

GMM Stage 1 Standard Errors
[[0.18442343        nan 0.04549784]
 [       nan 0.18642724        nan]
 [0.04549784        nan 0.16592603]]

      fun: 1.1077417432251324
 hess_inv: array([[ 0.01700604, -0.00110826,  0.00103492],
       [-0.00110826,  0.01737666, -0.00460659],
       [ 0.00103492, -0.00460659,  0.01376592]])
      jac: array([2.68220901e-07, 4.17232513e-07, 3.27825546e-07])
  message: 'Optimization terminated successfully.'
     nfev: 50
      nit: 8
     njev: 10
   status: 0
  success: True
        x: array([1.92104581, 1.09399361, 3.66145241])
Beta 0: 1.9210458074628922
Beta 1: 1.093993605947826
Beta 2: 3.661452412205812

GMM Stage 2 Variance
[[ 0.03401789 -0.0022231   0.00207841]
 [-0.0022231   0.03475173 -0.00921534]
 [ 0.00207841 -0.00921534  0.02752018]]

GMM Stage 2 Standard Errors
[[0.18443939        nan 0.04558958]
 [       nan 0.18641817        nan]
 [0.04558958        nan 0.16589207]]

GMM Variance Difference
[[ 1.59577010e-05             nan  9.17404363e-05]
 [            nan -9.06897682e-06             nan]
 [ 9.17404363e-05             nan -3.39656336e-05]]

GMM Standard Errors Difference
[[ 1.59577010e-05             nan  9.17404363e-05]
 [            nan -9.06897682e-06             nan]
 [ 9.17404363e-05             nan -3.39656336e-05]]
\end{lstlisting}

\end{document}